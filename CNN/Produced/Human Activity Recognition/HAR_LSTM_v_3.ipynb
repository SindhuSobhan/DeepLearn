{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "HAR_LSTM_v.3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cTSN-NfTrTy1",
        "Wx4MIeJUr1jw"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udk7TCjPrLWC",
        "colab_type": "text"
      },
      "source": [
        "#Human Activity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTSN-NfTrTy1",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zIH-uel3I6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_JxE6I73I6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activities are the class labels\n",
        "# It is a 6 class classification\n",
        "ACTIVITIES = {\n",
        "    0: 'WALKING',\n",
        "    1: 'WALKING_UPSTAIRS',\n",
        "    2: 'WALKING_DOWNSTAIRS',\n",
        "    3: 'SITTING',\n",
        "    4: 'STANDING',\n",
        "    5: 'LAYING',\n",
        "}\n",
        "\n",
        "# Utility function to print the confusion matrix\n",
        "def confusion_matrix(Y_true, Y_pred):\n",
        "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
        "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
        "\n",
        "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg_WovRV3I6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data directory\n",
        "DATADIR = 'UCI_HAR_Dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FaN9oQT3I6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Raw data signals\n",
        "# Signals are from Accelerometer and Gyroscope\n",
        "# The signals are in x,y,z directions\n",
        "# Sensor signals are filtered to have only body acceleration\n",
        "# excluding the acceleration due to gravity\n",
        "# Triaxial acceleration from the accelerometer is total acceleration\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJAN4xej3I61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'/content/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).as_matrix()\n",
        "        ) \n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myDnkclL3I64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_y(subset):\n",
        "    \"\"\"\n",
        "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
        "    that represents a human activity. We return a binary representation of \n",
        "    every sample objective as a 6 bits vector using One Hot Encoding\n",
        "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
        "    \"\"\"\n",
        "    filename = f'/content/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yr9l0z23I67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Obtain the dataset from multiple files.\n",
        "    Returns: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    y_train, y_test = load_y('train'), load_y('test')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al3jGTAK3I69",
        "colab_type": "code",
        "outputId": "06cb19f2-b3aa-4698-d565-f381acf2faf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# Importing tensorflow\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlYe2On3I6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuring a session\n",
        "session_conf = tf.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgfanI1z3I7C",
        "colab_type": "code",
        "outputId": "6c95914b-b340-4f36-da35-ae1440a029a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import Keras\n",
        "from keras import backend as K\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyo-ra0G3I7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABNBIhAZ3I7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6OOO-IKJ9KA",
        "colab_type": "code",
        "outputId": "59209320-1e7e-4697-823e-c2746f998cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pLpPUf6J1Is",
        "colab_type": "code",
        "outputId": "189e7279-b5c5-4094-ad98-f9000c27b397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import zipfile\n",
        "\n",
        "zipref = zipfile.ZipFile('/content/drive/My Drive/UCI_HAR_Dataset.zip', mode = 'r')\n",
        "zipref.extractall('/content/')\n",
        "zipref.close()\n",
        "# Loading the train and test data\n",
        "X_train, X_test, Y_train, Y_test = load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUHQomwkMQ_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUeKYM-3I7P",
        "colab_type": "code",
        "outputId": "c79b0b52-674c-40e5-9167-5594027181d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "timesteps = len(X_train[0])\n",
        "input_dim = len(X_train[0][0])\n",
        "n_classes = _count_classes(Y_train)\n",
        "\n",
        "print(timesteps)\n",
        "print(input_dim)\n",
        "print(len(X_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "9\n",
            "7352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSphiQn13I7S",
        "colab_type": "text"
      },
      "source": [
        "#Training\n",
        "####Training by means of LSTM model with one and two layers. One layer with 32 - 48 units typically yields performance of not more than 90 per cent. Increasing units and layers has been found to achieve performance of more than 93 perc ent on test set. \n",
        "#####Also, the test accuracy varies widly within an epoch for later epochs (After first 10), so model checkpoint is used to save the best model in each epoch and also the best amongst all epoch is saved after comparing with new generated model after each model. Ths way the file Best_model.hdf5 contains the best model at each epoch and is rewritten after an epoch. Also, the file HAR_Model.hdf5 contains the best model of all epochs. It gets overwritten is a better model than the already best model is discovered after an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tPSFV1S3I7T",
        "colab_type": "code",
        "outputId": "504a67cc-0cc4-448f-c8ef-3db8ac4b09e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "# Create a model object\n",
        "\n",
        "def create_model(n_hidden = 32, dropout = 0.5, n_layers = 1):\n",
        "  # Initiliazing the sequential model\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape = (timesteps, input_dim)))\n",
        "  \n",
        "  if n_layers > 1:\n",
        "    for layer in range(n_layers - 1):\n",
        "      # Configuring the parameters\n",
        "      model.add(LSTM(n_hidden, return_sequences = True))\n",
        "      # Adding a dropout layer\n",
        "      model.add(Dropout(dropout))\n",
        "    # Configuring the parameters\n",
        "    model.add(LSTM(units = n_hidden + 6))\n",
        "    # Adding a dropout layer\n",
        "    model.add(Dropout(dropout))  \n",
        "  else:\n",
        "    # Configuring the parameters\n",
        "    model.add(LSTM(units = n_hidden))\n",
        "    # Adding a dropout layer\n",
        "    model.add(Dropout(dropout))  \n",
        "    \n",
        "  # Adding a dense output layer with sigmoid activation\n",
        "  model.add(Dense(n_classes, activation='softmax'))\n",
        "  \n",
        "  # Compiling the mode\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "# Viewing model summary\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 32)                5376      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 5,574\n",
            "Trainable params: 5,574\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU8Ht92Z3I7Z",
        "colab_type": "code",
        "outputId": "c63860e1-281b-41b9-8d5f-75bd59efc10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##################################################################\n",
        "###############  Grid Search using Custom Code ##################\n",
        "##################################################################\n",
        "\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import itertools\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                               monitor = 'val_acc',\n",
        "                               factor = np.sqrt(0.8),\n",
        "                               cooldown = 0,\n",
        "                               patience = 2,\n",
        "                               min_lr = 1e-5)\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "  # For Linear Decay [1e-3 to 1e-5]\n",
        "  new_lr = 1e-3 - ((1e-3 - 1e-4) / 29) * epoch\n",
        "  \n",
        "  # For Hyperbolic Decay [1e-3 to 3.3e-5]\n",
        "  # new_lr = 1e-3 / (epoch + 1)\n",
        "  \n",
        "  # For Stepwise Decay\n",
        "  #if epoch % 10 == 5:\n",
        "  #  new_lr = lr * 0.6\n",
        "  \n",
        "  if new_lr < lr:\n",
        "    lr = new_lr\n",
        "    \n",
        "  print(\"For epoch {}, the LR is {}\".format(epoch + 1, lr)) \n",
        "  return lr\n",
        "\n",
        "#early_stopping = EarlyStopping(monitor = 'val_acc', patience = 7, verbose = 1, mode='auto')\n",
        "\n",
        "\n",
        "\n",
        "# Setting parameters\n",
        "dropout = [0.5, 0.7]\n",
        "n_hidden = [48, 64]\n",
        "n_layers = [2, 1]\n",
        "epochs = [30] # can be changed in later versions\n",
        "batch_size = [32] # can be changed in later versions\n",
        "\n",
        "\n",
        "params = {'epochs': [], 'n_layers' : [], 'n_hidden': [], 'dropout': [], 'batch_size' : [], 'acc': [0], 'val_acc' : [0]}\n",
        "\n",
        "\n",
        "for n_layers, n_hidden, dropout, epochs, batch_size in itertools.product(n_layers, n_hidden, dropout, epochs, batch_size):\n",
        "  \n",
        "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = '/content/drive/My Drive/Models/Best_model.hdf5', monitor = 'val_acc', verbose = 0, save_best_only = True)\n",
        "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "  model = create_model(n_hidden = n_hidden, dropout = dropout, n_layers = n_layers)\n",
        "  \n",
        "  print(\"For n_layers = {}, n_hidden = {}, dropout = {} and batch_size = {} ........................................\"\\\n",
        "        .format(n_layers, n_hidden, dropout, batch_size))\n",
        "\n",
        "  # Training the model\n",
        "  fit_model = model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size = batch_size,\n",
        "          validation_data = (X_test, Y_test),\n",
        "          epochs = epochs,\n",
        "          callbacks = [lr_reducer, checkpoint, lr_scheduler],\n",
        "          verbose = 2)\n",
        "  \n",
        "  model = load_model('/content/drive/My Drive/Models/Best_model.hdf5') \n",
        "  score_train = model.evaluate(X_train, Y_train)\n",
        "  score_val = model.evaluate(X_test, Y_test)\n",
        "  \n",
        "  params['epochs'].append(epochs)\n",
        "  params['n_layers'].append(n_layers)\n",
        "  params['n_hidden'].append(n_hidden)\n",
        "  params['dropout'].append(dropout)\n",
        "  params['batch_size'].append(batch_size)\n",
        "  params['acc'].append(score_train[1])\n",
        "  params['val_acc'].append(score_val[1])\n",
        "  \n",
        "  acc_train = params['acc']\n",
        "  acc_valid = params['val_acc']\n",
        "  \n",
        "  if acc_valid[-1] > np.max(acc_valid[:-1]):\n",
        "    model_best = model\n",
        "    param_best = {'epochs': epochs, 'n_layers' : n_layers, 'n_hidden': n_hidden, 'dropout': dropout, 'batch_size' : batch_size,\\\n",
        "                  'acc':acc_train[-1], 'val_acc' : acc_valid[-1]}\n",
        "    model_best.save('/content/drive/My Drive/Models/HAR_Model.hdf5')\n",
        "    print('Found better model, replacing earlier')\n",
        "    \n",
        "  print(\"With n_layers = {}, n_hidden= {} and dropout = {}, and batch_size = {}, the Train_Acc is = {} and Val_Acc is = {} \\n\\n\".\\\n",
        "        format(n_layers, n_hidden, dropout, batch_size, acc_train[-1], acc_valid[-1]))\n",
        "\n",
        "     \n",
        "# Display Pretty Table using PrettyTable library\n",
        "params['acc'].pop(0)\n",
        "params['val_acc'].pop(0)\n",
        "df = pd.DataFrame(params)  \n",
        "DF = PrettyTable()\n",
        "DF.field_names = df.columns\n",
        "for row in range(df.count().max()):\n",
        "  DF.add_row(df.iloc[row])\n",
        "print(DF)\n",
        "\n",
        "# Display best parameters\n",
        "print(\"The Best parameters are:\", param_best, sep = '\\n')\n",
        "                           "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For n_layers = 2, n_hidden = 48, dropout = 0.5 and batch_size = 32 ........................................\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 83s - loss: 1.0626 - acc: 0.5499 - val_loss: 0.8014 - val_acc: 0.6814\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 82s - loss: 0.7738 - acc: 0.7029 - val_loss: 0.6408 - val_acc: 0.7462\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 81s - loss: 0.4684 - acc: 0.8322 - val_loss: 0.5324 - val_acc: 0.8079\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 80s - loss: 0.3430 - acc: 0.8833 - val_loss: 0.4747 - val_acc: 0.8480\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 81s - loss: 0.2728 - acc: 0.9066 - val_loss: 0.5230 - val_acc: 0.8493\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 81s - loss: 0.2177 - acc: 0.9276 - val_loss: 0.4189 - val_acc: 0.8592\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 82s - loss: 0.1898 - acc: 0.9346 - val_loss: 0.3655 - val_acc: 0.8911\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 83s - loss: 0.1793 - acc: 0.9385 - val_loss: 0.3291 - val_acc: 0.8928\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 82s - loss: 0.1534 - acc: 0.9421 - val_loss: 0.4045 - val_acc: 0.8931\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 79s - loss: 0.1475 - acc: 0.9433 - val_loss: 0.3878 - val_acc: 0.8931\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 80s - loss: 0.1459 - acc: 0.9425 - val_loss: 0.2940 - val_acc: 0.9026\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 80s - loss: 0.1416 - acc: 0.9464 - val_loss: 0.5411 - val_acc: 0.8724\n",
            "For epoch 13, the LR is 0.0006275862068965517\n",
            "Epoch 13/30\n",
            "7352/7352 - 82s - loss: 0.1335 - acc: 0.9490 - val_loss: 0.4044 - val_acc: 0.9030\n",
            "For epoch 14, the LR is 0.000596551724137931\n",
            "Epoch 14/30\n",
            "7352/7352 - 82s - loss: 0.1248 - acc: 0.9513 - val_loss: 0.3668 - val_acc: 0.9026\n",
            "For epoch 15, the LR is 0.0005655172413793104\n",
            "Epoch 15/30\n",
            "7352/7352 - 82s - loss: 0.1239 - acc: 0.9480 - val_loss: 0.3756 - val_acc: 0.9009\n",
            "For epoch 16, the LR is 0.0005058139795437455\n",
            "Epoch 16/30\n",
            "7352/7352 - 81s - loss: 0.1275 - acc: 0.9479 - val_loss: 0.3623 - val_acc: 0.9070\n",
            "For epoch 17, the LR is 0.000503448275862069\n",
            "Epoch 17/30\n",
            "7352/7352 - 82s - loss: 0.1149 - acc: 0.9521 - val_loss: 0.3641 - val_acc: 0.9040\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 81s - loss: 0.1236 - acc: 0.9531 - val_loss: 0.4018 - val_acc: 0.9091\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 76s - loss: 0.1158 - acc: 0.9527 - val_loss: 0.4705 - val_acc: 0.9046\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 75s - loss: 0.1229 - acc: 0.9529 - val_loss: 0.3560 - val_acc: 0.9206\n",
            "For epoch 21, the LR is 0.0003793103448275862\n",
            "Epoch 21/30\n",
            "7352/7352 - 75s - loss: 0.1120 - acc: 0.9539 - val_loss: 0.3542 - val_acc: 0.9104\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 75s - loss: 0.1158 - acc: 0.9495 - val_loss: 0.4138 - val_acc: 0.9114\n",
            "For epoch 23, the LR is 0.0003115074068773538\n",
            "Epoch 23/30\n",
            "7352/7352 - 75s - loss: 0.1094 - acc: 0.9548 - val_loss: 0.3556 - val_acc: 0.9152\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 75s - loss: 0.1025 - acc: 0.9532 - val_loss: 0.3936 - val_acc: 0.9135\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 73s - loss: 0.1051 - acc: 0.9551 - val_loss: 0.4108 - val_acc: 0.9172\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 75s - loss: 0.1002 - acc: 0.9558 - val_loss: 0.4281 - val_acc: 0.9121\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 74s - loss: 0.1023 - acc: 0.9551 - val_loss: 0.4538 - val_acc: 0.9067\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 76s - loss: 0.0990 - acc: 0.9578 - val_loss: 0.4421 - val_acc: 0.9050\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 74s - loss: 0.1005 - acc: 0.9574 - val_loss: 0.4183 - val_acc: 0.9046\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 73s - loss: 0.0949 - acc: 0.9584 - val_loss: 0.4182 - val_acc: 0.9057\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "7352/7352 [==============================] - 18s 3ms/sample - loss: 0.0999 - acc: 0.9558\n",
            "2947/2947 [==============================] - 8s 3ms/sample - loss: 0.3560 - acc: 0.9206\n",
            "Found better model, replacing earlier\n",
            "With n_layers = 2, n_hidden= 48 and dropout = 0.5, and batch_size = 32, the Train_Acc is = 0.9557943344116211 and Val_Acc is = 0.9205971956253052 \n",
            "\n",
            "\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "For n_layers = 2, n_hidden = 48, dropout = 0.7 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 74s - loss: 1.1315 - acc: 0.5135 - val_loss: 0.8620 - val_acc: 0.6454\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 75s - loss: 0.6774 - acc: 0.7237 - val_loss: 0.6338 - val_acc: 0.7703\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 74s - loss: 0.4864 - acc: 0.8305 - val_loss: 0.6269 - val_acc: 0.7947\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 76s - loss: 0.3388 - acc: 0.8976 - val_loss: 0.4656 - val_acc: 0.8622\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 77s - loss: 0.3090 - acc: 0.9061 - val_loss: 0.3774 - val_acc: 0.8731\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 81s - loss: 0.2537 - acc: 0.9222 - val_loss: 0.5138 - val_acc: 0.8714\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 81s - loss: 0.2497 - acc: 0.9263 - val_loss: 1.2138 - val_acc: 0.7645\n",
            "For epoch 8, the LR is 0.000727878650650382\n",
            "Epoch 8/30\n",
            "7352/7352 - 81s - loss: 0.2477 - acc: 0.9279 - val_loss: 0.4028 - val_acc: 0.8806\n",
            "For epoch 9, the LR is 0.000727878650650382\n",
            "Epoch 9/30\n",
            "7352/7352 - 82s - loss: 0.2352 - acc: 0.9287 - val_loss: 0.3269 - val_acc: 0.8982\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 80s - loss: 0.1869 - acc: 0.9354 - val_loss: 0.3549 - val_acc: 0.8992\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 82s - loss: 0.1779 - acc: 0.9427 - val_loss: 0.5709 - val_acc: 0.8765\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 82s - loss: 0.1877 - acc: 0.9419 - val_loss: 0.3913 - val_acc: 0.9040\n",
            "For epoch 13, the LR is 0.0006275862068965517\n",
            "Epoch 13/30\n",
            "7352/7352 - 82s - loss: 0.1443 - acc: 0.9484 - val_loss: 0.4169 - val_acc: 0.9013\n",
            "For epoch 14, the LR is 0.000596551724137931\n",
            "Epoch 14/30\n",
            "7352/7352 - 81s - loss: 0.1433 - acc: 0.9450 - val_loss: 0.4573 - val_acc: 0.8928\n",
            "For epoch 15, the LR is 0.0005335721070878208\n",
            "Epoch 15/30\n",
            "7352/7352 - 81s - loss: 0.1596 - acc: 0.9438 - val_loss: 0.3756 - val_acc: 0.9152\n",
            "For epoch 16, the LR is 0.0005335721070878208\n",
            "Epoch 16/30\n",
            "7352/7352 - 82s - loss: 0.1381 - acc: 0.9450 - val_loss: 0.4191 - val_acc: 0.9101\n",
            "For epoch 17, the LR is 0.000503448275862069\n",
            "Epoch 17/30\n",
            "7352/7352 - 81s - loss: 0.1357 - acc: 0.9480 - val_loss: 0.4415 - val_acc: 0.9111\n",
            "For epoch 18, the LR is 0.00045029784087091684\n",
            "Epoch 18/30\n",
            "7352/7352 - 79s - loss: 0.1275 - acc: 0.9475 - val_loss: 0.3691 - val_acc: 0.9145\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 76s - loss: 0.1273 - acc: 0.9470 - val_loss: 0.3877 - val_acc: 0.9118\n",
            "For epoch 20, the LR is 0.0003947816730942577\n",
            "Epoch 20/30\n",
            "7352/7352 - 77s - loss: 0.1235 - acc: 0.9478 - val_loss: 0.3551 - val_acc: 0.9152\n",
            "For epoch 21, the LR is 0.0003793103448275862\n",
            "Epoch 21/30\n",
            "7352/7352 - 76s - loss: 0.1196 - acc: 0.9483 - val_loss: 0.3864 - val_acc: 0.9148\n",
            "For epoch 22, the LR is 0.0003392654762137681\n",
            "Epoch 22/30\n",
            "7352/7352 - 76s - loss: 0.1158 - acc: 0.9491 - val_loss: 0.3689 - val_acc: 0.9186\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 74s - loss: 0.1151 - acc: 0.9531 - val_loss: 0.4188 - val_acc: 0.9158\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 75s - loss: 0.1160 - acc: 0.9508 - val_loss: 0.4165 - val_acc: 0.9277\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 75s - loss: 0.1111 - acc: 0.9529 - val_loss: 0.3886 - val_acc: 0.9206\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 75s - loss: 0.1050 - acc: 0.9536 - val_loss: 0.4709 - val_acc: 0.9175\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 75s - loss: 0.1091 - acc: 0.9557 - val_loss: 0.4048 - val_acc: 0.9189\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 76s - loss: 0.1040 - acc: 0.9547 - val_loss: 0.4394 - val_acc: 0.9243\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 74s - loss: 0.1034 - acc: 0.9535 - val_loss: 0.4091 - val_acc: 0.9257\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 77s - loss: 0.1047 - acc: 0.9517 - val_loss: 0.4503 - val_acc: 0.9250\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "7352/7352 [==============================] - 19s 3ms/sample - loss: 0.0988 - acc: 0.9536\n",
            "2947/2947 [==============================] - 8s 3ms/sample - loss: 0.4165 - acc: 0.9277\n",
            "Found better model, replacing earlier\n",
            "With n_layers = 2, n_hidden= 48 and dropout = 0.7, and batch_size = 32, the Train_Acc is = 0.953618049621582 and Val_Acc is = 0.9277231097221375 \n",
            "\n",
            "\n",
            "For n_layers = 2, n_hidden = 64, dropout = 0.5 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 77s - loss: 1.0179 - acc: 0.5688 - val_loss: 0.7568 - val_acc: 0.7184\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 75s - loss: 0.6259 - acc: 0.7554 - val_loss: 0.9070 - val_acc: 0.6895\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 75s - loss: 0.4390 - acc: 0.8507 - val_loss: 0.5275 - val_acc: 0.8232\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 79s - loss: 0.3226 - acc: 0.8921 - val_loss: 0.4474 - val_acc: 0.8510\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 83s - loss: 0.2605 - acc: 0.9127 - val_loss: 0.4744 - val_acc: 0.8609\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 80s - loss: 0.2095 - acc: 0.9286 - val_loss: 0.3792 - val_acc: 0.8887\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 80s - loss: 0.1787 - acc: 0.9348 - val_loss: 0.3718 - val_acc: 0.8873\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 83s - loss: 0.1705 - acc: 0.9384 - val_loss: 0.3412 - val_acc: 0.8955\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 82s - loss: 0.1476 - acc: 0.9399 - val_loss: 0.3205 - val_acc: 0.9050\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 82s - loss: 0.1395 - acc: 0.9468 - val_loss: 0.3862 - val_acc: 0.8921\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 82s - loss: 0.1323 - acc: 0.9464 - val_loss: 0.2802 - val_acc: 0.9050\n",
            "For epoch 12, the LR is 0.0006168463733047247\n",
            "Epoch 12/30\n",
            "7352/7352 - 81s - loss: 0.1303 - acc: 0.9499 - val_loss: 0.3441 - val_acc: 0.9043\n",
            "For epoch 13, the LR is 0.0006168463733047247\n",
            "Epoch 13/30\n",
            "7352/7352 - 81s - loss: 0.1184 - acc: 0.9501 - val_loss: 0.3434 - val_acc: 0.9138\n",
            "For epoch 14, the LR is 0.000596551724137931\n",
            "Epoch 14/30\n",
            "7352/7352 - 82s - loss: 0.1225 - acc: 0.9533 - val_loss: 0.3404 - val_acc: 0.8985\n",
            "For epoch 15, the LR is 0.0005655172413793104\n",
            "Epoch 15/30\n",
            "7352/7352 - 82s - loss: 0.1192 - acc: 0.9543 - val_loss: 0.3862 - val_acc: 0.9084\n",
            "For epoch 16, the LR is 0.0005058139795437455\n",
            "Epoch 16/30\n",
            "7352/7352 - 82s - loss: 0.1143 - acc: 0.9532 - val_loss: 0.3367 - val_acc: 0.9063\n",
            "For epoch 17, the LR is 0.000503448275862069\n",
            "Epoch 17/30\n",
            "7352/7352 - 81s - loss: 0.1192 - acc: 0.9551 - val_loss: 0.3042 - val_acc: 0.9128\n",
            "For epoch 18, the LR is 0.00045029784087091684\n",
            "Epoch 18/30\n",
            "7352/7352 - 81s - loss: 0.1102 - acc: 0.9547 - val_loss: 0.3342 - val_acc: 0.9080\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 82s - loss: 0.1091 - acc: 0.9550 - val_loss: 0.3138 - val_acc: 0.9101\n",
            "For epoch 20, the LR is 0.0003947816730942577\n",
            "Epoch 20/30\n",
            "7352/7352 - 82s - loss: 0.1096 - acc: 0.9538 - val_loss: 0.2961 - val_acc: 0.9182\n",
            "For epoch 21, the LR is 0.0003793103448275862\n",
            "Epoch 21/30\n",
            "7352/7352 - 83s - loss: 0.1032 - acc: 0.9562 - val_loss: 0.3316 - val_acc: 0.9182\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 83s - loss: 0.1060 - acc: 0.9566 - val_loss: 0.2879 - val_acc: 0.9158\n",
            "For epoch 23, the LR is 0.0003115074068773538\n",
            "Epoch 23/30\n",
            "7352/7352 - 84s - loss: 0.1024 - acc: 0.9585 - val_loss: 0.3416 - val_acc: 0.9070\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 85s - loss: 0.0989 - acc: 0.9593 - val_loss: 0.3498 - val_acc: 0.9135\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 84s - loss: 0.1012 - acc: 0.9608 - val_loss: 0.3339 - val_acc: 0.9179\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 85s - loss: 0.1004 - acc: 0.9593 - val_loss: 0.2709 - val_acc: 0.9270\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 83s - loss: 0.0991 - acc: 0.9604 - val_loss: 0.2626 - val_acc: 0.9247\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 84s - loss: 0.0951 - acc: 0.9606 - val_loss: 0.2711 - val_acc: 0.9287\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 85s - loss: 0.0966 - acc: 0.9591 - val_loss: 0.2958 - val_acc: 0.9264\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 84s - loss: 0.0975 - acc: 0.9601 - val_loss: 0.2887 - val_acc: 0.9321\n",
            "7352/7352 [==============================] - 22s 3ms/sample - loss: 0.0887 - acc: 0.9621\n",
            "2947/2947 [==============================] - 9s 3ms/sample - loss: 0.2887 - acc: 0.9321\n",
            "Found better model, replacing earlier\n",
            "With n_layers = 2, n_hidden= 64 and dropout = 0.5, and batch_size = 32, the Train_Acc is = 0.9620511531829834 and Val_Acc is = 0.9321343898773193 \n",
            "\n",
            "\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "For n_layers = 2, n_hidden = 64, dropout = 0.7 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 83s - loss: 1.1039 - acc: 0.5314 - val_loss: 0.8450 - val_acc: 0.6474\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 82s - loss: 0.6898 - acc: 0.7289 - val_loss: 0.6893 - val_acc: 0.7492\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 84s - loss: 0.4910 - acc: 0.8301 - val_loss: 0.5293 - val_acc: 0.8086\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 82s - loss: 0.3924 - acc: 0.8761 - val_loss: 0.5104 - val_acc: 0.8314\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 84s - loss: 0.3137 - acc: 0.9027 - val_loss: 0.4384 - val_acc: 0.8649\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 82s - loss: 0.2492 - acc: 0.9184 - val_loss: 0.5678 - val_acc: 0.8497\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 82s - loss: 0.1855 - acc: 0.9334 - val_loss: 0.5173 - val_acc: 0.8717\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 84s - loss: 0.1779 - acc: 0.9421 - val_loss: 0.4808 - val_acc: 0.8836\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 84s - loss: 0.1640 - acc: 0.9366 - val_loss: 0.6857 - val_acc: 0.8677\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 87s - loss: 0.1434 - acc: 0.9487 - val_loss: 0.4637 - val_acc: 0.8992\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 85s - loss: 0.1435 - acc: 0.9471 - val_loss: 0.5760 - val_acc: 0.8968\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 85s - loss: 0.1450 - acc: 0.9478 - val_loss: 0.4376 - val_acc: 0.8867\n",
            "For epoch 13, the LR is 0.0005890882457606494\n",
            "Epoch 13/30\n",
            "7352/7352 - 85s - loss: 0.1247 - acc: 0.9514 - val_loss: 0.5246 - val_acc: 0.9074\n",
            "For epoch 14, the LR is 0.0005890882457606494\n",
            "Epoch 14/30\n",
            "7352/7352 - 85s - loss: 0.1307 - acc: 0.9512 - val_loss: 0.6554 - val_acc: 0.9006\n",
            "For epoch 15, the LR is 0.0005655172413793104\n",
            "Epoch 15/30\n",
            "7352/7352 - 85s - loss: 0.1330 - acc: 0.9510 - val_loss: 0.4560 - val_acc: 0.9053\n",
            "For epoch 16, the LR is 0.0005058139795437455\n",
            "Epoch 16/30\n",
            "7352/7352 - 85s - loss: 0.1173 - acc: 0.9544 - val_loss: 0.4764 - val_acc: 0.9084\n",
            "For epoch 17, the LR is 0.000503448275862069\n",
            "Epoch 17/30\n",
            "7352/7352 - 85s - loss: 0.1162 - acc: 0.9535 - val_loss: 0.6321 - val_acc: 0.8989\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 84s - loss: 0.1245 - acc: 0.9542 - val_loss: 0.6041 - val_acc: 0.9019\n",
            "For epoch 19, the LR is 0.00042253974243067205\n",
            "Epoch 19/30\n",
            "7352/7352 - 83s - loss: 0.1219 - acc: 0.9548 - val_loss: 0.5845 - val_acc: 0.9094\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 83s - loss: 0.1152 - acc: 0.9532 - val_loss: 0.6468 - val_acc: 0.9013\n",
            "For epoch 21, the LR is 0.0003793103448275862\n",
            "Epoch 21/30\n",
            "7352/7352 - 82s - loss: 0.1160 - acc: 0.9527 - val_loss: 0.8125 - val_acc: 0.8873\n",
            "For epoch 22, the LR is 0.0003392654762137681\n",
            "Epoch 22/30\n",
            "7352/7352 - 85s - loss: 0.1146 - acc: 0.9535 - val_loss: 0.5982 - val_acc: 0.8996\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 84s - loss: 0.1077 - acc: 0.9551 - val_loss: 0.6406 - val_acc: 0.9053\n",
            "For epoch 24, the LR is 0.000283749308437109\n",
            "Epoch 24/30\n",
            "7352/7352 - 82s - loss: 0.1065 - acc: 0.9563 - val_loss: 0.7499 - val_acc: 0.9060\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 84s - loss: 0.1071 - acc: 0.9572 - val_loss: 0.6343 - val_acc: 0.9002\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 82s - loss: 0.1063 - acc: 0.9576 - val_loss: 0.6400 - val_acc: 0.9074\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 83s - loss: 0.1012 - acc: 0.9578 - val_loss: 0.7027 - val_acc: 0.8992\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 82s - loss: 0.1015 - acc: 0.9574 - val_loss: 0.6159 - val_acc: 0.9101\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 81s - loss: 0.0991 - acc: 0.9574 - val_loss: 0.6824 - val_acc: 0.9084\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 81s - loss: 0.1049 - acc: 0.9584 - val_loss: 0.6667 - val_acc: 0.9067\n",
            "7352/7352 [==============================] - 21s 3ms/sample - loss: 0.0899 - acc: 0.9591\n",
            "2947/2947 [==============================] - 8s 3ms/sample - loss: 0.6159 - acc: 0.9101\n",
            "With n_layers = 2, n_hidden= 64 and dropout = 0.7, and batch_size = 32, the Train_Acc is = 0.9590587615966797 and Val_Acc is = 0.9100780487060547 \n",
            "\n",
            "\n",
            "For n_layers = 1, n_hidden = 48, dropout = 0.5 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 43s - loss: 1.1812 - acc: 0.5045 - val_loss: 1.0297 - val_acc: 0.5803\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 41s - loss: 0.8682 - acc: 0.6300 - val_loss: 0.8329 - val_acc: 0.6624\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 41s - loss: 0.7393 - acc: 0.6632 - val_loss: 0.7214 - val_acc: 0.7075\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 42s - loss: 0.6038 - acc: 0.7462 - val_loss: 0.5982 - val_acc: 0.7679\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 41s - loss: 0.4785 - acc: 0.8124 - val_loss: 0.5695 - val_acc: 0.7876\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 41s - loss: 0.3904 - acc: 0.8648 - val_loss: 0.4634 - val_acc: 0.8168\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 42s - loss: 0.2805 - acc: 0.9108 - val_loss: 0.3890 - val_acc: 0.8595\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 43s - loss: 0.2467 - acc: 0.9217 - val_loss: 0.3339 - val_acc: 0.8887\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 42s - loss: 0.2055 - acc: 0.9286 - val_loss: 0.3503 - val_acc: 0.8802\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 40s - loss: 0.1807 - acc: 0.9396 - val_loss: 0.3108 - val_acc: 0.8924\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 42s - loss: 0.1643 - acc: 0.9419 - val_loss: 0.2888 - val_acc: 0.8924\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 41s - loss: 0.1611 - acc: 0.9427 - val_loss: 0.4001 - val_acc: 0.8870\n",
            "For epoch 13, the LR is 0.0005890882457606494\n",
            "Epoch 13/30\n",
            "7352/7352 - 41s - loss: 0.1521 - acc: 0.9431 - val_loss: 0.3259 - val_acc: 0.8894\n",
            "For epoch 14, the LR is 0.0005890882457606494\n",
            "Epoch 14/30\n",
            "7352/7352 - 42s - loss: 0.1496 - acc: 0.9449 - val_loss: 0.3205 - val_acc: 0.9006\n",
            "For epoch 15, the LR is 0.0005655172413793104\n",
            "Epoch 15/30\n",
            "7352/7352 - 41s - loss: 0.1412 - acc: 0.9445 - val_loss: 0.4079 - val_acc: 0.8911\n",
            "For epoch 16, the LR is 0.0005344827586206897\n",
            "Epoch 16/30\n",
            "7352/7352 - 41s - loss: 0.1427 - acc: 0.9478 - val_loss: 0.3596 - val_acc: 0.9002\n",
            "For epoch 17, the LR is 0.0004780559102073312\n",
            "Epoch 17/30\n",
            "7352/7352 - 42s - loss: 0.1310 - acc: 0.9527 - val_loss: 0.3909 - val_acc: 0.8945\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 42s - loss: 0.1271 - acc: 0.9509 - val_loss: 0.3809 - val_acc: 0.9009\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 41s - loss: 0.1262 - acc: 0.9516 - val_loss: 0.3443 - val_acc: 0.8982\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 42s - loss: 0.1356 - acc: 0.9494 - val_loss: 0.3361 - val_acc: 0.8975\n",
            "For epoch 21, the LR is 0.0003670235746540129\n",
            "Epoch 21/30\n",
            "7352/7352 - 42s - loss: 0.1197 - acc: 0.9520 - val_loss: 0.3640 - val_acc: 0.9013\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 42s - loss: 0.1234 - acc: 0.9518 - val_loss: 0.3364 - val_acc: 0.9050\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 42s - loss: 0.1172 - acc: 0.9506 - val_loss: 0.4032 - val_acc: 0.9033\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 41s - loss: 0.1220 - acc: 0.9516 - val_loss: 0.3672 - val_acc: 0.8975\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 42s - loss: 0.1176 - acc: 0.9527 - val_loss: 0.3383 - val_acc: 0.9077\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 42s - loss: 0.1130 - acc: 0.9558 - val_loss: 0.3477 - val_acc: 0.8996\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 42s - loss: 0.1149 - acc: 0.9543 - val_loss: 0.3486 - val_acc: 0.9080\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 42s - loss: 0.1096 - acc: 0.9569 - val_loss: 0.3168 - val_acc: 0.9141\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 41s - loss: 0.1059 - acc: 0.9563 - val_loss: 0.3450 - val_acc: 0.9070\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 43s - loss: 0.1044 - acc: 0.9580 - val_loss: 0.3321 - val_acc: 0.9104\n",
            "7352/7352 [==============================] - 12s 2ms/sample - loss: 0.1088 - acc: 0.9573\n",
            "2947/2947 [==============================] - 4s 1ms/sample - loss: 0.3168 - acc: 0.9141\n",
            "With n_layers = 1, n_hidden= 48 and dropout = 0.5, and batch_size = 32, the Train_Acc is = 0.957290530204773 and Val_Acc is = 0.9141499996185303 \n",
            "\n",
            "\n",
            "For n_layers = 1, n_hidden = 48, dropout = 0.7 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 45s - loss: 1.2630 - acc: 0.4551 - val_loss: 1.0644 - val_acc: 0.5582\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 41s - loss: 0.8855 - acc: 0.6096 - val_loss: 0.8009 - val_acc: 0.6532\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 42s - loss: 0.7518 - acc: 0.6428 - val_loss: 0.7311 - val_acc: 0.6709\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 41s - loss: 0.6996 - acc: 0.6817 - val_loss: 0.6785 - val_acc: 0.7323\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 41s - loss: 0.6046 - acc: 0.7448 - val_loss: 0.6251 - val_acc: 0.7910\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 42s - loss: 0.5410 - acc: 0.8154 - val_loss: 0.5861 - val_acc: 0.8130\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 41s - loss: 0.4133 - acc: 0.8610 - val_loss: 0.5206 - val_acc: 0.8381\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 43s - loss: 0.3414 - acc: 0.8976 - val_loss: 0.6156 - val_acc: 0.8079\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 42s - loss: 0.2917 - acc: 0.9132 - val_loss: 0.4250 - val_acc: 0.8666\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 44s - loss: 0.2663 - acc: 0.9217 - val_loss: 0.3335 - val_acc: 0.8829\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 42s - loss: 0.2288 - acc: 0.9261 - val_loss: 0.3808 - val_acc: 0.8707\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 44s - loss: 0.2166 - acc: 0.9301 - val_loss: 0.3615 - val_acc: 0.8819\n",
            "For epoch 13, the LR is 0.0005890882457606494\n",
            "Epoch 13/30\n",
            "7352/7352 - 43s - loss: 0.1969 - acc: 0.9329 - val_loss: 0.3171 - val_acc: 0.8924\n",
            "For epoch 14, the LR is 0.0005890882457606494\n",
            "Epoch 14/30\n",
            "7352/7352 - 42s - loss: 0.2039 - acc: 0.9358 - val_loss: 0.4122 - val_acc: 0.8931\n",
            "For epoch 15, the LR is 0.0005655172413793104\n",
            "Epoch 15/30\n",
            "7352/7352 - 42s - loss: 0.1859 - acc: 0.9382 - val_loss: 0.3992 - val_acc: 0.8880\n",
            "For epoch 16, the LR is 0.0005344827586206897\n",
            "Epoch 16/30\n",
            "7352/7352 - 42s - loss: 0.1859 - acc: 0.9406 - val_loss: 0.3690 - val_acc: 0.8935\n",
            "For epoch 17, the LR is 0.000503448275862069\n",
            "Epoch 17/30\n",
            "7352/7352 - 42s - loss: 0.1848 - acc: 0.9421 - val_loss: 0.3432 - val_acc: 0.8968\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 41s - loss: 0.1683 - acc: 0.9463 - val_loss: 0.3627 - val_acc: 0.8972\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 42s - loss: 0.1565 - acc: 0.9418 - val_loss: 0.3945 - val_acc: 0.8958\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 41s - loss: 0.1588 - acc: 0.9452 - val_loss: 0.4495 - val_acc: 0.8955\n",
            "For epoch 21, the LR is 0.0003670235746540129\n",
            "Epoch 21/30\n",
            "7352/7352 - 42s - loss: 0.1401 - acc: 0.9486 - val_loss: 0.4227 - val_acc: 0.8992\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 41s - loss: 0.1512 - acc: 0.9456 - val_loss: 0.3546 - val_acc: 0.9009\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 41s - loss: 0.1371 - acc: 0.9479 - val_loss: 0.4302 - val_acc: 0.8999\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 43s - loss: 0.1412 - acc: 0.9498 - val_loss: 0.3935 - val_acc: 0.8989\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 40s - loss: 0.1376 - acc: 0.9486 - val_loss: 0.3738 - val_acc: 0.9033\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 38s - loss: 0.1283 - acc: 0.9494 - val_loss: 0.3693 - val_acc: 0.9026\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 40s - loss: 0.1243 - acc: 0.9531 - val_loss: 0.4270 - val_acc: 0.9019\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 40s - loss: 0.1269 - acc: 0.9516 - val_loss: 0.4548 - val_acc: 0.8982\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 39s - loss: 0.1206 - acc: 0.9536 - val_loss: 0.4161 - val_acc: 0.9030\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 41s - loss: 0.1188 - acc: 0.9536 - val_loss: 0.4069 - val_acc: 0.9036\n",
            "7352/7352 [==============================] - 12s 2ms/sample - loss: 0.0981 - acc: 0.9539\n",
            "2947/2947 [==============================] - 5s 2ms/sample - loss: 0.4069 - acc: 0.9036\n",
            "With n_layers = 1, n_hidden= 48 and dropout = 0.7, and batch_size = 32, the Train_Acc is = 0.9538900852203369 and Val_Acc is = 0.903630793094635 \n",
            "\n",
            "\n",
            "For n_layers = 1, n_hidden = 64, dropout = 0.5 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 44s - loss: 1.1365 - acc: 0.5102 - val_loss: 0.8841 - val_acc: 0.6712\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 41s - loss: 0.7309 - acc: 0.6843 - val_loss: 0.6588 - val_acc: 0.7496\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 42s - loss: 0.5775 - acc: 0.7750 - val_loss: 0.7906 - val_acc: 0.7340\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 40s - loss: 0.4523 - acc: 0.8347 - val_loss: 0.5922 - val_acc: 0.8029\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 42s - loss: 0.3381 - acc: 0.8919 - val_loss: 0.4538 - val_acc: 0.8571\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 41s - loss: 0.2562 - acc: 0.9163 - val_loss: 0.4370 - val_acc: 0.8673\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 43s - loss: 0.2298 - acc: 0.9238 - val_loss: 0.3496 - val_acc: 0.8836\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 41s - loss: 0.1864 - acc: 0.9329 - val_loss: 0.3714 - val_acc: 0.8880\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 41s - loss: 0.1894 - acc: 0.9361 - val_loss: 0.3061 - val_acc: 0.8918\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 41s - loss: 0.1644 - acc: 0.9402 - val_loss: 0.2759 - val_acc: 0.8989\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 42s - loss: 0.1596 - acc: 0.9414 - val_loss: 0.3266 - val_acc: 0.9026\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 42s - loss: 0.1524 - acc: 0.9445 - val_loss: 0.2929 - val_acc: 0.9070\n",
            "For epoch 13, the LR is 0.0006275862068965517\n",
            "Epoch 13/30\n",
            "7352/7352 - 42s - loss: 0.1421 - acc: 0.9467 - val_loss: 0.2951 - val_acc: 0.8921\n",
            "For epoch 14, the LR is 0.000596551724137931\n",
            "Epoch 14/30\n",
            "7352/7352 - 42s - loss: 0.1414 - acc: 0.9483 - val_loss: 0.3851 - val_acc: 0.8812\n",
            "For epoch 15, the LR is 0.0005335721070878208\n",
            "Epoch 15/30\n",
            "7352/7352 - 42s - loss: 0.1514 - acc: 0.9478 - val_loss: 0.2827 - val_acc: 0.9063\n",
            "For epoch 16, the LR is 0.0005335721070878208\n",
            "Epoch 16/30\n",
            "7352/7352 - 43s - loss: 0.1341 - acc: 0.9464 - val_loss: 0.3374 - val_acc: 0.8962\n",
            "For epoch 17, the LR is 0.00047724141040816903\n",
            "Epoch 17/30\n",
            "7352/7352 - 42s - loss: 0.1322 - acc: 0.9476 - val_loss: 0.2497 - val_acc: 0.9074\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 43s - loss: 0.1198 - acc: 0.9540 - val_loss: 0.2707 - val_acc: 0.9114\n",
            "For epoch 19, the LR is 0.0004413793103448275\n",
            "Epoch 19/30\n",
            "7352/7352 - 45s - loss: 0.1187 - acc: 0.9520 - val_loss: 0.3122 - val_acc: 0.9030\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 43s - loss: 0.1190 - acc: 0.9514 - val_loss: 0.2884 - val_acc: 0.9101\n",
            "For epoch 21, the LR is 0.0003670235746540129\n",
            "Epoch 21/30\n",
            "7352/7352 - 43s - loss: 0.1190 - acc: 0.9525 - val_loss: 0.3603 - val_acc: 0.8951\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 44s - loss: 0.1184 - acc: 0.9536 - val_loss: 0.2806 - val_acc: 0.9179\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 42s - loss: 0.1116 - acc: 0.9550 - val_loss: 0.3476 - val_acc: 0.9125\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 43s - loss: 0.1124 - acc: 0.9540 - val_loss: 0.3588 - val_acc: 0.9108\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 42s - loss: 0.1111 - acc: 0.9543 - val_loss: 0.3665 - val_acc: 0.9094\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 43s - loss: 0.1223 - acc: 0.9536 - val_loss: 0.3735 - val_acc: 0.9070\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 41s - loss: 0.1060 - acc: 0.9563 - val_loss: 0.3700 - val_acc: 0.9094\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 41s - loss: 0.1040 - acc: 0.9572 - val_loss: 0.3811 - val_acc: 0.9125\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 43s - loss: 0.1004 - acc: 0.9570 - val_loss: 0.3970 - val_acc: 0.9080\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 42s - loss: 0.0976 - acc: 0.9592 - val_loss: 0.4348 - val_acc: 0.9063\n",
            "7352/7352 [==============================] - 12s 2ms/sample - loss: 0.1018 - acc: 0.9577\n",
            "2947/2947 [==============================] - 4s 1ms/sample - loss: 0.2806 - acc: 0.9179\n",
            "With n_layers = 1, n_hidden= 64 and dropout = 0.5, and batch_size = 32, the Train_Acc is = 0.9576985836029053 and Val_Acc is = 0.9178826212882996 \n",
            "\n",
            "\n",
            "For n_layers = 1, n_hidden = 64, dropout = 0.7 and batch_size = 32 ........................................\n",
            "Train on 7352 samples, validate on 2947 samples\n",
            "For epoch 1, the LR is 0.001\n",
            "Epoch 1/30\n",
            "7352/7352 - 47s - loss: 1.2256 - acc: 0.4780 - val_loss: 1.1260 - val_acc: 0.5090\n",
            "For epoch 2, the LR is 0.0009689655172413793\n",
            "Epoch 2/30\n",
            "7352/7352 - 42s - loss: 0.9098 - acc: 0.6236 - val_loss: 0.8009 - val_acc: 0.6851\n",
            "For epoch 3, the LR is 0.0009379310344827586\n",
            "Epoch 3/30\n",
            "7352/7352 - 42s - loss: 0.7107 - acc: 0.7180 - val_loss: 0.6703 - val_acc: 0.7642\n",
            "For epoch 4, the LR is 0.000906896551724138\n",
            "Epoch 4/30\n",
            "7352/7352 - 41s - loss: 0.5701 - acc: 0.7946 - val_loss: 0.5602 - val_acc: 0.8100\n",
            "For epoch 5, the LR is 0.0008758620689655173\n",
            "Epoch 5/30\n",
            "7352/7352 - 42s - loss: 0.4673 - acc: 0.8575 - val_loss: 0.6190 - val_acc: 0.8069\n",
            "For epoch 6, the LR is 0.0008448275862068965\n",
            "Epoch 6/30\n",
            "7352/7352 - 42s - loss: 0.4494 - acc: 0.8736 - val_loss: 0.5180 - val_acc: 0.8415\n",
            "For epoch 7, the LR is 0.0008137931034482759\n",
            "Epoch 7/30\n",
            "7352/7352 - 42s - loss: 0.3344 - acc: 0.9011 - val_loss: 0.4478 - val_acc: 0.8636\n",
            "For epoch 8, the LR is 0.0007827586206896552\n",
            "Epoch 8/30\n",
            "7352/7352 - 44s - loss: 0.3257 - acc: 0.9042 - val_loss: 0.5059 - val_acc: 0.8256\n",
            "For epoch 9, the LR is 0.0007517241379310345\n",
            "Epoch 9/30\n",
            "7352/7352 - 42s - loss: 0.2470 - acc: 0.9222 - val_loss: 0.4220 - val_acc: 0.8748\n",
            "For epoch 10, the LR is 0.0007206896551724138\n",
            "Epoch 10/30\n",
            "7352/7352 - 42s - loss: 0.2185 - acc: 0.9319 - val_loss: 0.5491 - val_acc: 0.8602\n",
            "For epoch 11, the LR is 0.000689655172413793\n",
            "Epoch 11/30\n",
            "7352/7352 - 40s - loss: 0.2062 - acc: 0.9308 - val_loss: 0.3384 - val_acc: 0.8999\n",
            "For epoch 12, the LR is 0.0006586206896551724\n",
            "Epoch 12/30\n",
            "7352/7352 - 42s - loss: 0.2017 - acc: 0.9327 - val_loss: 0.4616 - val_acc: 0.8694\n",
            "For epoch 13, the LR is 0.0006275862068965517\n",
            "Epoch 13/30\n",
            "7352/7352 - 43s - loss: 0.1897 - acc: 0.9384 - val_loss: 0.4023 - val_acc: 0.8928\n",
            "For epoch 14, the LR is 0.0005613301764242351\n",
            "Epoch 14/30\n",
            "7352/7352 - 41s - loss: 0.1674 - acc: 0.9419 - val_loss: 0.4538 - val_acc: 0.8758\n",
            "For epoch 15, the LR is 0.0005613301764242351\n",
            "Epoch 15/30\n",
            "7352/7352 - 43s - loss: 0.1722 - acc: 0.9426 - val_loss: 0.4372 - val_acc: 0.8904\n",
            "For epoch 16, the LR is 0.0005020689568482339\n",
            "Epoch 16/30\n",
            "7352/7352 - 43s - loss: 0.1566 - acc: 0.9441 - val_loss: 0.3439 - val_acc: 0.9087\n",
            "For epoch 17, the LR is 0.0005020689568482339\n",
            "Epoch 17/30\n",
            "7352/7352 - 41s - loss: 0.1545 - acc: 0.9463 - val_loss: 0.4099 - val_acc: 0.9002\n",
            "For epoch 18, the LR is 0.00047241379310344824\n",
            "Epoch 18/30\n",
            "7352/7352 - 41s - loss: 0.1573 - acc: 0.9479 - val_loss: 0.2821 - val_acc: 0.8989\n",
            "For epoch 19, the LR is 0.00042253974243067205\n",
            "Epoch 19/30\n",
            "7352/7352 - 41s - loss: 0.1356 - acc: 0.9487 - val_loss: 0.4113 - val_acc: 0.9043\n",
            "For epoch 20, the LR is 0.0004103448275862069\n",
            "Epoch 20/30\n",
            "7352/7352 - 42s - loss: 0.1634 - acc: 0.9453 - val_loss: 0.4065 - val_acc: 0.9013\n",
            "For epoch 21, the LR is 0.0003670235746540129\n",
            "Epoch 21/30\n",
            "7352/7352 - 43s - loss: 0.1300 - acc: 0.9490 - val_loss: 0.4346 - val_acc: 0.8965\n",
            "For epoch 22, the LR is 0.00034827586206896547\n",
            "Epoch 22/30\n",
            "7352/7352 - 42s - loss: 0.1290 - acc: 0.9516 - val_loss: 0.3615 - val_acc: 0.9104\n",
            "For epoch 23, the LR is 0.00031724137931034486\n",
            "Epoch 23/30\n",
            "7352/7352 - 42s - loss: 0.1320 - acc: 0.9516 - val_loss: 0.3619 - val_acc: 0.9036\n",
            "For epoch 24, the LR is 0.00028620689655172414\n",
            "Epoch 24/30\n",
            "7352/7352 - 44s - loss: 0.1321 - acc: 0.9491 - val_loss: 0.3655 - val_acc: 0.9104\n",
            "For epoch 25, the LR is 0.0002551724137931034\n",
            "Epoch 25/30\n",
            "7352/7352 - 42s - loss: 0.1211 - acc: 0.9527 - val_loss: 0.4239 - val_acc: 0.8972\n",
            "For epoch 26, the LR is 0.0002241379310344827\n",
            "Epoch 26/30\n",
            "7352/7352 - 42s - loss: 0.1171 - acc: 0.9508 - val_loss: 0.4729 - val_acc: 0.8890\n",
            "For epoch 27, the LR is 0.00019310344827586198\n",
            "Epoch 27/30\n",
            "7352/7352 - 42s - loss: 0.1188 - acc: 0.9524 - val_loss: 0.4425 - val_acc: 0.9036\n",
            "For epoch 28, the LR is 0.00016206896551724137\n",
            "Epoch 28/30\n",
            "7352/7352 - 44s - loss: 0.1146 - acc: 0.9546 - val_loss: 0.3779 - val_acc: 0.9067\n",
            "For epoch 29, the LR is 0.00013103448275862066\n",
            "Epoch 29/30\n",
            "7352/7352 - 45s - loss: 0.1128 - acc: 0.9525 - val_loss: 0.4361 - val_acc: 0.8982\n",
            "For epoch 30, the LR is 9.999999999999994e-05\n",
            "Epoch 30/30\n",
            "7352/7352 - 44s - loss: 0.1135 - acc: 0.9531 - val_loss: 0.4059 - val_acc: 0.8968\n",
            "7352/7352 [==============================] - 13s 2ms/sample - loss: 0.1028 - acc: 0.9542\n",
            "2947/2947 [==============================] - 5s 2ms/sample - loss: 0.3615 - acc: 0.9104\n",
            "With n_layers = 1, n_hidden= 64 and dropout = 0.7, and batch_size = 32, the Train_Acc is = 0.9541621208190918 and Val_Acc is = 0.910417377948761 \n",
            "\n",
            "\n",
            "+--------+----------+----------+---------+------------+--------------------+--------------------+\n",
            "| epochs | n_layers | n_hidden | dropout | batch_size |        acc         |      val_acc       |\n",
            "+--------+----------+----------+---------+------------+--------------------+--------------------+\n",
            "|  30.0  |   2.0    |   48.0   |   0.5   |    32.0    | 0.9557943344116211 | 0.9205971956253052 |\n",
            "|  30.0  |   2.0    |   48.0   |   0.7   |    32.0    | 0.953618049621582  | 0.9277231097221375 |\n",
            "|  30.0  |   2.0    |   64.0   |   0.5   |    32.0    | 0.9620511531829834 | 0.9321343898773193 |\n",
            "|  30.0  |   2.0    |   64.0   |   0.7   |    32.0    | 0.9590587615966797 | 0.9100780487060547 |\n",
            "|  30.0  |   1.0    |   48.0   |   0.5   |    32.0    | 0.957290530204773  | 0.9141499996185303 |\n",
            "|  30.0  |   1.0    |   48.0   |   0.7   |    32.0    | 0.9538900852203369 | 0.903630793094635  |\n",
            "|  30.0  |   1.0    |   64.0   |   0.5   |    32.0    | 0.9576985836029053 | 0.9178826212882996 |\n",
            "|  30.0  |   1.0    |   64.0   |   0.7   |    32.0    | 0.9541621208190918 | 0.910417377948761  |\n",
            "+--------+----------+----------+---------+------------+--------------------+--------------------+\n",
            "The Best parameters are:\n",
            "{'epochs': 30, 'n_layers': 2, 'n_hidden': 64, 'dropout': 0.5, 'batch_size': 32, 'acc': 0.96205115, 'val_acc': 0.9321344}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7NM8fbJZ8PT",
        "colab_type": "code",
        "outputId": "634f434f-7c8b-46aa-a438-1f178e8840ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Load and test overall best model\n",
        "model_best = load_model('/content/drive/My Drive/Models/HAR_Model.hdf5')\n",
        "model_best.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2947/2947 [==============================] - 10s 3ms/sample - loss: 0.2887 - acc: 0.9321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2886682210203786, 0.9321344]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SN8r9ciHowH",
        "colab_type": "code",
        "outputId": "a11d1b4c-8447-4ea2-9576-3a7d2b3594f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#All Parameters\n",
        "params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.95579433,\n",
              "  0.95361805,\n",
              "  0.96205115,\n",
              "  0.95905876,\n",
              "  0.95729053,\n",
              "  0.9538901,\n",
              "  0.9576986,\n",
              "  0.9541621],\n",
              " 'batch_size': [32, 32, 32, 32, 32, 32, 32, 32],\n",
              " 'dropout': [0.5, 0.7, 0.5, 0.7, 0.5, 0.7, 0.5, 0.7],\n",
              " 'epochs': [30, 30, 30, 30, 30, 30, 30, 30],\n",
              " 'n_hidden': [48, 48, 64, 64, 48, 48, 64, 64],\n",
              " 'n_layers': [2, 2, 2, 2, 1, 1, 1, 1],\n",
              " 'val_acc': [0.9205972,\n",
              "  0.9277231,\n",
              "  0.9321344,\n",
              "  0.91007805,\n",
              "  0.91415,\n",
              "  0.9036308,\n",
              "  0.9178826,\n",
              "  0.9104174]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrY4oIDV3I7f",
        "colab_type": "code",
        "outputId": "bf140e43-3a42-4af5-836f-4f71b9ecc633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Confusion Matrix\n",
        "pd.set_option('display.max_columns', 30)\n",
        "print(confusion_matrix(Y_test, model_best.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
            "True                                                                         \n",
            "LAYING                 536        0         0        0                   0   \n",
            "SITTING                  5      399        85        0                   0   \n",
            "STANDING                 0       67       462        2                   0   \n",
            "WALKING                  0        0         0      473                  10   \n",
            "WALKING_DOWNSTAIRS       0        0         0        0                 417   \n",
            "WALKING_UPSTAIRS         0        0         0        6                   5   \n",
            "\n",
            "Pred                WALKING_UPSTAIRS  \n",
            "True                                  \n",
            "LAYING                             1  \n",
            "SITTING                            2  \n",
            "STANDING                           1  \n",
            "WALKING                           13  \n",
            "WALKING_DOWNSTAIRS                 3  \n",
            "WALKING_UPSTAIRS                 460  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKyeqvfP7Rtz",
        "colab_type": "code",
        "outputId": "4105ff0a-52f7-44b2-c1e5-33fd8bce1f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Confusion Matrix\n",
        "df = confusion_matrix(Y_test, model_best.predict(X_test)) \n",
        "DF = PrettyTable()\n",
        "DF.field_names = df.columns\n",
        "for row in range(df.count().max()):\n",
        "  DF.add_row(df.iloc[row])\n",
        "print(DF)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------+----------+---------+--------------------+------------------+\n",
            "| LAYING | SITTING | STANDING | WALKING | WALKING_DOWNSTAIRS | WALKING_UPSTAIRS |\n",
            "+--------+---------+----------+---------+--------------------+------------------+\n",
            "|  536   |    0    |    0     |    0    |         0          |        1         |\n",
            "|   5    |   399   |    85    |    0    |         0          |        2         |\n",
            "|   0    |    67   |   462    |    2    |         0          |        1         |\n",
            "|   0    |    0    |    0     |   473   |         10         |        13        |\n",
            "|   0    |    0    |    0     |    0    |        417         |        3         |\n",
            "|   0    |    0    |    0     |    6    |         5          |       460        |\n",
            "+--------+---------+----------+---------+--------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4MIeJUr1jw",
        "colab_type": "text"
      },
      "source": [
        "#FineTuning\n",
        "####(To be used scarcely. Does not show much improvement when used in this case.)\n",
        "#####Tried various means, such as decreasing LR for another 30 epochs, training only the last Dense layer weights, Adding another dense layer and training it only, training everything apart from the first LSTM layer and so on. But very little improvement was observed. Although on one instance an accuracy of 93.3 per cent was obtained as an improvement from 92 per cent. Maybe checkpoints on loss ought to yield better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHWUWnERr558",
        "colab_type": "code",
        "outputId": "5c8d75d4-073e-4065-dbd9-a9345dea5026",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Upload best model\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-37c37e75-eaa5-4add-8515-bd6f30bf4794\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-37c37e75-eaa5-4add-8515-bd6f30bf4794\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving HAR_Model_2.hdf5 to HAR_Model_2.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKbQs35GuRVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Best Model. Name has to be changed accordingly.\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/HAR_Model_2.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzXkfHwqvCcc",
        "colab_type": "code",
        "outputId": "740a0e6a-d2d5-46c5-a97a-691d8fe68c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 128, 48)           11136     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128, 48)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 54)                22248     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 54)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 330       \n",
            "=================================================================\n",
            "Total params: 44,850\n",
            "Trainable params: 33,714\n",
            "Non-trainable params: 11,136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYrJGc_YvGui",
        "colab_type": "code",
        "outputId": "f5f5aa6f-243d-4fbc-e353-17a89a53c44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2947/2947 [==============================] - 10s 3ms/sample - loss: 0.3821 - acc: 0.9240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38214929368716866, 0.9239905]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHJBt2xXjyM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make first LSTM layer untrainable\n",
        "model.layers[0].trainable = False\n",
        "new_model = tensorflow.keras.Sequential([model])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNf2RCqUkPgA",
        "colab_type": "code",
        "outputId": "15ec245d-fdff-4d09-c081-14fdf2aa99b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_4 (Sequential)    (None, 6)                 33714     \n",
            "=================================================================\n",
            "Total params: 33,714\n",
            "Trainable params: 22,578\n",
            "Non-trainable params: 11,136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lkv6SILlVyr9",
        "colab": {}
      },
      "source": [
        "#Adding a new dense layer if required\n",
        "\n",
        "#from tensorflow.keras.models import Model\n",
        "#from tensorflow.keras.layers import Dense\n",
        "#import tensorflow\n",
        "\n",
        "#model.trainable = False\n",
        "#for layer in model.layers[2:]:\n",
        "#  layer.trainable = True\n",
        "  \n",
        "\n",
        "\n",
        "#new_model = tensorflow.keras.Sequential(\n",
        "#[\n",
        "#    model,\n",
        "#    Dense(6, activation = 'softmax')\n",
        "#])\n",
        "\n",
        "#new_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fsrCHcO7G-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "new_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= SGD(lr = 1e-4),\n",
        "              metrics=['accuracy'])\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCBaAaNz2wlf",
        "colab_type": "code",
        "outputId": "79f92559-f57e-47e3-c60b-cebcc66f0966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                               monitor = 'val_acc',\n",
        "                               factor = np.sqrt(0.8),\n",
        "                               cooldown = 0,\n",
        "                               patience = 2,\n",
        "                               min_lr = 1e-6)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = '/content/drive/My Drive/Models/HAR_model_fine_tuned.hdf5', monitor = 'val_acc', verbose = 0, save_best_only = True)\n",
        "\n",
        "\n",
        "fit_model = new_model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size = 16,\n",
        "          validation_data = (X_test, Y_test),\n",
        "          epochs = 60,\n",
        "          initial_epoch = 30,\n",
        "          callbacks = [lr_reducer, checkpoint],\n",
        "          verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7352 samples, validate on 2947 samples\n",
            "Epoch 31/60\n",
            "7352/7352 [==============================] - 70s 9ms/sample - loss: 0.0982 - acc: 0.9517 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 32/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0979 - acc: 0.9546 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 33/60\n",
            "7352/7352 [==============================] - 66s 9ms/sample - loss: 0.0972 - acc: 0.9538 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 34/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0981 - acc: 0.9544 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 35/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0983 - acc: 0.9516 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 36/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0981 - acc: 0.9528 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 37/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0984 - acc: 0.9538 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 38/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0976 - acc: 0.9533 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 39/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0980 - acc: 0.9527 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 40/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0966 - acc: 0.9535 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 41/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0974 - acc: 0.9536 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 42/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0984 - acc: 0.9531 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 43/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.1000 - acc: 0.9517 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 44/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0975 - acc: 0.9539 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 45/60\n",
            "7352/7352 [==============================] - 64s 9ms/sample - loss: 0.0993 - acc: 0.9498 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 46/60\n",
            "7352/7352 [==============================] - 64s 9ms/sample - loss: 0.0977 - acc: 0.9551 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 47/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0974 - acc: 0.9525 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 48/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0999 - acc: 0.9517 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 49/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0972 - acc: 0.9514 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 50/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0979 - acc: 0.9528 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 51/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0984 - acc: 0.9514 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 52/60\n",
            "7352/7352 [==============================] - 66s 9ms/sample - loss: 0.0981 - acc: 0.9521 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 53/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.1002 - acc: 0.9528 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 54/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0989 - acc: 0.9517 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 55/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0983 - acc: 0.9521 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 56/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0994 - acc: 0.9501 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 57/60\n",
            "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.0967 - acc: 0.9543 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 58/60\n",
            "7352/7352 [==============================] - 64s 9ms/sample - loss: 0.0990 - acc: 0.9527 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 59/60\n",
            "7352/7352 [==============================] - 64s 9ms/sample - loss: 0.0980 - acc: 0.9550 - val_loss: 0.3572 - val_acc: 0.9379\n",
            "Epoch 60/60\n",
            "7352/7352 [==============================] - 64s 9ms/sample - loss: 0.0973 - acc: 0.9533 - val_loss: 0.3572 - val_acc: 0.9379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw6MLWZe6ex7",
        "colab_type": "code",
        "outputId": "89dbdabf-ff2b-4239-f2db-bbd3ead040e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "fit_model.model.optimizer.get_config()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amsgrad': False,\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'decay': 0.0,\n",
              " 'epsilon': 1e-07,\n",
              " 'learning_rate': 2.6214399e-05,\n",
              " 'name': 'Adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GfX5US-sJeK",
        "colab_type": "code",
        "outputId": "d23b2d1e-048a-4ef1-84ca-bc1e64162414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/Models/HAR_model_fine_tuned.hdf5')\n",
        "\n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2947/2947 [==============================] - 10s 4ms/sample - loss: 0.3572 - acc: 0.9379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35716692178012577, 0.9379029]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    }
  ]
}